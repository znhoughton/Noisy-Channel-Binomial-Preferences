---
title: "noisy-channel-binom-pref-analysis"
author: "Zachary Houghton"
date: "2025-01-19"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(brms)

#install.packages("./cplDataAnalysis_0.1.tar.gz", repos = NULL,type="source")
library(cplDataAnalysis)
```

## Load Data

```{r}
#pilot_data = read.csv('../Data/pilot_results3.csv', quote = "\"")
data = read.csv('../Data/data.csv', quote = "\"")


#pilot_data$date = as.Date(pilot_data$date)

data$date = as.Date(data$date)
# data = pilot_data %>%
#   filter(date > as.Date("2025-01-24"))

# pilot = pilot_data2 %>%
#   select('participant', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
#   filter(!is.na(List)) %>%
#   mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
#   group_by(participant) %>%
#   mutate('OverallAcc' = mean(correct))

data_analysis = data %>%
  dplyr::select('PROLIFIC_PID', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
  #filter(!is.na(List)) %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  rename('participant' = 'PROLIFIC_PID') %>%
  group_by(participant) %>%
  mutate('OverallAcc' = mean(correct)) 
  


audio_questions_key = read_csv('../Data/audio_questions.csv')
#audio_questions_key$Item = factor(audio_questions_key$Item)


data_analysis = data_analysis %>%
  left_join(audio_questions_key)

audio_only = data_analysis %>%
  filter(Question_about_audio == '1') %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  group_by(participant) %>%
  mutate('AudioAcc' = mean(correct)) %>%
  slice_head(n=1) %>%
  select(participant, AudioAcc)

data_analysis = data_analysis %>%
  left_join(audio_only)

# 

# data_analysis2 = pilot %>%
#   full_join(data_analysis)

# data_analysis = data_analysis2

data_analysis = data_analysis %>%
  mutate(sentence_split = str_split(Sentence, " "))

data_analysis = data_analysis %>%
  rowwise() %>%  # Ensures per-row operation
  mutate(target_word_index = case_when(
    SentenceType == 'Sentence (alphabetical)' ~ which(sentence_split == WordA)[1],
    SentenceType == 'Sentence (nonalphabetical)' ~ which(sentence_split == WordB)[1],
    TRUE ~ NA_real_
  )) %>%
  ungroup()  # Remove rowwise grouping




data_analysis = data_analysis %>%
  mutate(target_word_index = target_word_index - 1)



data_analysis1 = data_analysis %>%
  pivot_longer(cols = matches("^Word[0-9]+$"), names_to = 'word_index', names_prefix = 'Word', values_to = 'reading_time')



data_analysis1 = data_analysis1 %>%
  mutate(
    words = str_split(Sentence, "\\s+"),  # Split sentences into words
    extracted_word = mapply(function(word_list, idx) {
      adj_idx = as.numeric(idx) + 1  # Adjust for 0-based index
      if (adj_idx > length(word_list) | adj_idx < 1) NA else word_list[adj_idx]
    }, words, word_index)
  ) %>%
  select(-words)  # Drop the intermediate list column

data_analysis1$word_index = as.numeric(data_analysis1$word_index)
data_analysis1$target_word_index = as.numeric(data_analysis1$target_word_index)
data_analysis1$reading_time = data_analysis1$reading_time * 1000 #conver to ms

data_analysis1 = data_analysis1 %>%
  ungroup() %>%
  mutate('target' = ifelse(word_index >= target_word_index & word_index <= (target_word_index + 2), 1, 0)) %>%
  ungroup() %>%
  mutate('spillover' = ifelse(word_index >= (target_word_index + 3) & word_index <= (target_word_index + 5), 1, 0)) 

data_analysis1 = data_analysis1 %>%
  mutate(word_len = nchar(extracted_word))

data_analysis1 = data_analysis1 %>%
  filter(!is.na(reading_time))

data_analysis1$participant = factor(data_analysis1$participant)
data_analysis1$Item = factor(data_analysis1$Item)
data_analysis1$rt = data_analysis1$reading_time

data_analysis1 = data_analysis1 %>%
  filter(extracted_word != '+')



data_analysis1 <- data_analysis1 %>%
  arrange(Item, word_index) %>%
  group_by(Item) %>%
  slice(1:(n() - 1)) %>%  # Selects all but the last row per group
  ungroup()

data_analysis1$participant = factor(data_analysis1$participant)

residual_rts = data.frame(residual_rts = res.rt(data_analysis1, subj.name = 'participant', wordlen.name = 'word_len'))

data_analysis1$residual_rts = residual_rts$residual_rts


# 
# 
# summary_data = data_analysis1 %>%
#   group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct) %>%
#   summarize(
#     target_rt = sum(residual_rts[target == 1], na.rm = TRUE),
#     spillover_rt = sum(residual_rts[spillover == 1], na.rm = TRUE),
#     target_len = sum(word_len[target == 1], na.rm = T),
#     spillover_len = sum(word_len[spillover==1], na.rm = T),
#     .groups = "drop" 
#   )
# 
# summary_data = summary_data %>%
#   ungroup() %>%
#   group_by(participant, Item) %>%
#   pivot_longer(cols = c('target_rt', 'spillover_rt', 'target_len', 'spillover_len'),
#                names_to = c("region", ".value"),
#                names_pattern = "(.+)_(.+)")


# code to get residual rts
# res.rt <- function(dat, subj.name='subj', wordlen.name='wordlen',
#                         rt.name='rt', verbose=T) {
#   if(! is.element(wordlen.name,names(dat)))
#     stop(paste("Error: your data frame doesn't contain a compoment for wordlength named",wordlen.name))
#     dat <- data.frame(subj = dat[[subj.name]], wordlen = dat[[wordlen.name]],
#                       rt = dat[[rt.name]])
#     length.slope <- c() # just to see what typical slope is
#     res.rt <- c() # full list of residual rts
# 
#     for(i in levels(dat$subj)) {
#       wordlen.subj <- dat$wordlen[dat$subj==i]
#       if(length(wordlen.subj) == 0)
#         next
#       rt.subj <- dat$rt[dat$subj==i]
#       model <- lm(rt.subj~wordlen.subj)
#       res.rt[dat$subj==i] <- residuals(model)
#       length.slope <- c(length.slope, model$coef["wordlen.subj"])
#     }
#     mean.slope <- mean(length.slope)
#     se.slope <- se(length.slope)
#     if (verbose) {
#       cat(sprintf('The mean slope was %.1f with an se of %.1f.\n',
#                   mean.slope, se.slope))
#     }
#     return(res.rt)
# }




```

```{r}

key = read_csv('../Data/key.csv')

key$Item = factor(key$Item)

  
data_analysis1 = data_analysis1 %>%
  left_join(key) %>%
  filter(prime_freq != 'frequent') 
  
length(unique(data_analysis1$Item))

  
```

```{r}

data_analysis1$prime_freq = factor(data_analysis1$prime_freq, levels = c('unrelated', 'not-frequent'))
data_analysis1$target_freq = factor(data_analysis1$target_freq, levels = c('not-frequent', 'frequent'))

data_analysis_first_exp = data_analysis1
```

## Analyses

### Overall Acc \> 70%

```{r}

target_region_data = data_analysis1 %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup() %>%
  group_by(participant) %>%
  mutate(ExpAcc = mean(correct))

sum(target_region_data['ExpAcc'] > 0.7)

mean_rts = target_region_data %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = target_region_data %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

target_region_data = target_region_data %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

target_region_data$prime_freq = relevel(target_region_data$prime_freq, ref = 'unrelated')
target_region_data$target_freq = relevel(target_region_data$target_freq, ref = 'not-frequent')

target_region_data$participant = factor(target_region_data$participant)
target_region_data$Item = factor(target_region_data$Item)

length(unique(target_region_data$participant))
length(unique(target_region_data$Item))

rows_per_participant = target_region_data %>% #sanity check
  group_by(participant) %>%
  summarize(n())
#using 0 + Intercept syntax following: https://discourse.mc-stan.org/t/understanding-intercept-prior-in-brms/34027
m1 = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = target_region_data,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(
                #set_prior("student_t(3, 0, 30)", class = "Intercept"),
                set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.95),
         file = 'full_m1' #accuracy_greater_90
)

fixef(m1)
conditional_effects(m1)

percent_greater_zero = data.frame(fixef(m1, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero

#fixef_df = data.frame(conditional_effects(m1)[1], plot=F)
```

### Only Binomial, No Spillover

```{r}
only_binomials = data_analysis1 %>% 
  filter(target == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup()

mean_rts = only_binomials %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = only_binomials %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])


only_binomials = only_binomials %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

m2 = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = only_binomials,
          iter = 20000, 
          warmup = 10000,
          chains = 4,
          core = 4,
          prior = c(
                #set_prior("student_t(3, 0, 30)", class = "Intercept"),
                set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.95),
         file = 'full_m2' #full_m2
)

fixef(m2)
conditional_effects(m1)

percent_greater_zero = data.frame(fixef(m2, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero
```

### Audio Acc \> 70%

```{r}

audio_acc_greater_70 = data_analysis1 %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup()

mean_rts = audio_acc_greater_70 %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = audio_acc_greater_70 %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

audio_acc_greater_70 = audio_acc_greater_70 %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

audio_acc_greater_70$prime_freq = relevel(audio_acc_greater_70$prime_freq, ref = 'unrelated')
audio_acc_greater_70$target_freq = relevel(audio_acc_greater_70$target_freq, ref = 'not-frequent')

audio_acc_greater_70$participant = factor(audio_acc_greater_70$participant)
audio_acc_greater_70$Item = factor(audio_acc_greater_70$Item)

length(unique(audio_acc_greater_70$participant))
length(unique(audio_acc_greater_70$Item))


audio_acc_70 = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = audio_acc_greater_70,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70' #accuracy_greater_90
)

fixef(audio_acc_70)
conditional_effects(audio_acc_70)

percent_greater_zero = data.frame(fixef(audio_acc_70, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero
```

### Audio Corr as Fixed-Effect

```{r}
target_region_data = data_analysis1 %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup() %>%
  group_by(participant) %>%
  mutate(ExpAcc = mean(correct))

sum(target_region_data['ExpAcc'] > 0.7)

mean_rts = target_region_data %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = target_region_data %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

target_region_data = target_region_data %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

target_region_data$prime_freq = relevel(target_region_data$prime_freq, ref = 'unrelated')
target_region_data$target_freq = relevel(target_region_data$target_freq, ref = 'not-frequent')

target_region_data$participant = factor(target_region_data$participant)
target_region_data$Item = factor(target_region_data$Item)

#target_region_data$correct = factor

length(unique(target_region_data$participant))
length(unique(target_region_data$Item))

rows_per_participant = target_region_data %>% #sanity check
  group_by(participant) %>%
  summarize(n())
#using 0 + Intercept syntax following: https://discourse.mc-stan.org/t/understanding-intercept-prior-in-brms/34027
m_audio_fixed_effect = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq * correct + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = target_region_data,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(
                #set_prior("student_t(3, 0, 30)", class = "Intercept"),
                set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.95),
         file = 'corr_as_fixed_effect' #accuracy_greater_90
)

fixef(m_audio_fixed_effect)
conditional_effects(m_audio_fixed_effect)

percent_greater_zero = data.frame(fixef(m_audio_fixed_effect, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero

```

## Unrelated vs Frequet Prime

```{r}
#pilot_data = read.csv('../Data/pilot_results3.csv', quote = "\"")
data = read.csv('../Data/data.csv', quote = "\"")


#pilot_data$date = as.Date(pilot_data$date)

data$date = as.Date(data$date)
# data = pilot_data %>%
#   filter(date > as.Date("2025-01-24"))

# pilot = pilot_data2 %>%
#   select('participant', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
#   filter(!is.na(List)) %>%
#   mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
#   group_by(participant) %>%
#   mutate('OverallAcc' = mean(correct))

data_analysis = data %>%
  dplyr::select('PROLIFIC_PID', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
  #filter(!is.na(List)) %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  rename('participant' = 'PROLIFIC_PID') %>%
  group_by(participant) %>%
  mutate('OverallAcc' = mean(correct)) 
  


audio_questions_key = read_csv('../Data/audio_questions.csv')
#audio_questions_key$Item = factor(audio_questions_key$Item)


data_analysis = data_analysis %>%
  left_join(audio_questions_key)

audio_only = data_analysis %>%
  filter(Question_about_audio == '1') %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  group_by(participant) %>%
  mutate('AudioAcc' = mean(correct)) %>%
  slice_head(n=1) %>%
  select(participant, AudioAcc)

data_analysis = data_analysis %>%
  left_join(audio_only)

# 

# data_analysis2 = pilot %>%
#   full_join(data_analysis)

# data_analysis = data_analysis2

data_analysis = data_analysis %>%
  mutate(sentence_split = str_split(Sentence, " "))

data_analysis = data_analysis %>%
  rowwise() %>%  # Ensures per-row operation
  mutate(target_word_index = case_when(
    SentenceType == 'Sentence (alphabetical)' ~ which(sentence_split == WordA)[1],
    SentenceType == 'Sentence (nonalphabetical)' ~ which(sentence_split == WordB)[1],
    TRUE ~ NA_real_
  )) %>%
  ungroup()  # Remove rowwise grouping




data_analysis = data_analysis %>%
  mutate(target_word_index = target_word_index - 1)



data_analysis1_freq = data_analysis %>%
  pivot_longer(cols = matches("^Word[0-9]+$"), names_to = 'word_index', names_prefix = 'Word', values_to = 'reading_time')



data_analysis1_freq = data_analysis1_freq %>%
  mutate(
    words = str_split(Sentence, "\\s+"),  # Split sentences into words
    extracted_word = mapply(function(word_list, idx) {
      adj_idx = as.numeric(idx) + 1  # Adjust for 0-based index
      if (adj_idx > length(word_list) | adj_idx < 1) NA else word_list[adj_idx]
    }, words, word_index)
  ) %>%
  select(-words)  # Drop the intermediate list column

data_analysis1_freq$word_index = as.numeric(data_analysis1_freq$word_index)
data_analysis1_freq$target_word_index = as.numeric(data_analysis1_freq$target_word_index)
data_analysis1_freq$reading_time = data_analysis1_freq$reading_time * 1000 #conver to ms

data_analysis1_freq = data_analysis1_freq %>%
  ungroup() %>%
  mutate('target' = ifelse(word_index >= target_word_index & word_index <= (target_word_index + 2), 1, 0)) %>%
  ungroup() %>%
  mutate('spillover' = ifelse(word_index >= (target_word_index + 3) & word_index <= (target_word_index + 5), 1, 0)) 

data_analysis1_freq = data_analysis1_freq %>%
  mutate(word_len = nchar(extracted_word))

data_analysis1_freq = data_analysis1_freq %>%
  filter(!is.na(reading_time))

data_analysis1_freq$participant = factor(data_analysis1_freq$participant)
data_analysis1_freq$Item = factor(data_analysis1_freq$Item)
data_analysis1_freq$rt = data_analysis1_freq$reading_time

data_analysis1_freq = data_analysis1_freq %>%
  filter(extracted_word != '+')



data_analysis1_freq <- data_analysis1_freq %>%
  arrange(Item, word_index) %>%
  group_by(Item) %>%
  slice(1:(n() - 1)) %>%  # Selects all but the last row per group
  ungroup()

data_analysis1_freq$participant = factor(data_analysis1_freq$participant)

residual_rts = data.frame(residual_rts = res.rt(data_analysis1_freq, subj.name = 'participant', wordlen.name = 'word_len'))

data_analysis1_freq$residual_rts = residual_rts$residual_rts

key = read_csv('../Data/key.csv')

key$Item = factor(key$Item)

  
data_analysis1_freq = data_analysis1_freq %>%
  left_join(key) %>%
  filter(prime_freq != 'not-frequent') 
  
length(unique(data_analysis1_freq$Item))



data_analysis1_freq$prime_freq = factor(data_analysis1_freq$prime_freq, levels = c('unrelated', 'frequent'))
data_analysis1_freq$target_freq = factor(data_analysis1_freq$target_freq, levels = c('not-frequent', 'frequent'))




```

### Overall Acc \> 0.7

```{r}

target_region_data_freq = data_analysis1_freq %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup() %>%
  group_by(participant) %>%
  mutate(ExpAcc = mean(correct))

sum(target_region_data_freq['ExpAcc'] > 0.7)

mean_rts = target_region_data_freq %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = target_region_data_freq %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

target_region_data_freq = target_region_data_freq %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

target_region_data_freq$prime_freq = relevel(target_region_data_freq$prime_freq, ref = 'unrelated')
target_region_data_freq$target_freq = relevel(target_region_data_freq$target_freq, ref = 'not-frequent')

target_region_data_freq$participant = factor(target_region_data_freq$participant)
target_region_data_freq$Item = factor(target_region_data_freq$Item)

length(unique(target_region_data_freq$participant))
length(unique(target_region_data_freq$Item))

rows_per_participant = target_region_data_freq %>% #sanity check
  group_by(participant) %>%
  summarize(n())
#using 0 + Intercept syntax following: https://discourse.mc-stan.org/t/understanding-intercept-prior-in-brms/34027
m1_freq = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = target_region_data_freq,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(
                #set_prior("student_t(3, 0, 30)", class = "Intercept"),
                set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.95),
         file = 'full_m1_freq' #accuracy_greater_90
)

fixef(m1_freq)
conditional_effects(m1_freq)

percent_greater_zero = data.frame(fixef(m1_freq, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero
```

### Audio Acc \> 0.7

```{r}

audio_acc_greater_70_freq = data_analysis1_freq %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup()

mean_rts = audio_acc_greater_70_freq %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = audio_acc_greater_70_freq %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

audio_acc_greater_70_freq = audio_acc_greater_70_freq %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

audio_acc_greater_70_freq$prime_freq = relevel(audio_acc_greater_70_freq$prime_freq, ref = 'unrelated')
audio_acc_greater_70_freq$target_freq = relevel(audio_acc_greater_70_freq$target_freq, ref = 'not-frequent')

audio_acc_greater_70_freq$participant = factor(audio_acc_greater_70_freq$participant)
audio_acc_greater_70_freq$Item = factor(audio_acc_greater_70_freq$Item)

length(unique(audio_acc_greater_70_freq$participant))
length(unique(audio_acc_greater_70_freq$Item))


audio_acc_70_freq = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = audio_acc_greater_70_freq,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_freq' #accuracy_greater_90
)

fixef(audio_acc_70_freq)
conditional_effects(audio_acc_70_freq)

percent_greater_zero = data.frame(fixef(audio_acc_70_freq, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero
```

## Examining performance on mental rotation task

```{r}

mental_rotation_data = data %>% 
  select(PROLIFIC_PID, Prime, Sentence, angle, right_im, key_resp.corr, audio_loop.thisTrialN, audio_loop.thisIndex, audio_loop.thisN) %>%
  mutate(Sentence = na_if(Sentence, "")) %>%
  group_by(PROLIFIC_PID) %>%
  fill(Sentence, .direction = 'up')

mental_rotation_data = mental_rotation_data %>%
  group_by(PROLIFIC_PID, Sentence) %>%
  mutate(trial = audio_loop.thisTrialN) %>%
  fill(trial, .direction = 'up') 


mental_rotation_summary = mental_rotation_data %>%
  group_by(PROLIFIC_PID, Sentence, trial) %>%
  summarize(mean_angle = mean(angle, na.rm = T), mean_corr = mean(key_resp.corr, na.rm = T), num_rotations = n())
```

```{r}
mental_rotation_summary = mental_rotation_summary %>%
  rename(participant = PROLIFIC_PID)

data_analysis1_mental_rotation = data_analysis1 %>%
  left_join(mental_rotation_summary, by = c('participant', 'Sentence'))


```

```{r}
data_analysis1_mental_rotation = data_analysis1_mental_rotation %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc, mean_angle, mean_corr, num_rotations) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup() %>%
  group_by(participant) %>%
  mutate(ExpAcc = mean(correct))

sum(data_analysis1_mental_rotation['ExpAcc'] > 0.7)

mean_rts = data_analysis1_mental_rotation %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = data_analysis1_mental_rotation %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

data_analysis1_mental_rotation = data_analysis1_mental_rotation %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

data_analysis1_mental_rotation$prime_freq = relevel(data_analysis1_mental_rotation$prime_freq, ref = 'unrelated')
data_analysis1_mental_rotation$target_freq = relevel(data_analysis1_mental_rotation$target_freq, ref = 'not-frequent')

data_analysis1_mental_rotation$participant = factor(data_analysis1_mental_rotation$participant)
data_analysis1_mental_rotation$Item = factor(data_analysis1_mental_rotation$Item)

length(unique(data_analysis1_mental_rotation$participant))
length(unique(data_analysis1_mental_rotation$Item))

rows_per_participant = data_analysis1_mental_rotation %>% #sanity check
  group_by(participant) %>%
  summarize(n())
```

```{r}
for_plots = na.omit(data_analysis1_mental_rotation)
plot(for_plots$mean_angle, for_plots$mean_corr)
```

```{r}
easy_trials = for_plots %>%
  filter(mean_angle < 90)

easy_trials_model = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = easy_trials,
          iter = 6000, 
          warmup = 3000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'easy_trials_model' #accuracy_greater_90
)

fixef(easy_trials_model)
conditional_effects(easy_trials_model)
```

```{r}
percent_greater_zero = data.frame(fixef(easy_trials_model, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero



```
### frequent vs unrelated 

```{r}

mental_rotation_data = data %>% 
  select(PROLIFIC_PID, Prime, Sentence, angle, right_im, key_resp.corr, audio_loop.thisTrialN, audio_loop.thisIndex, audio_loop.thisN) %>%
  mutate(Sentence = na_if(Sentence, "")) %>%
  group_by(PROLIFIC_PID) %>%
  fill(Sentence, .direction = 'up')

mental_rotation_data = mental_rotation_data %>%
  group_by(PROLIFIC_PID, Sentence) %>%
  mutate(trial = audio_loop.thisTrialN) %>%
  fill(trial, .direction = 'up') 


mental_rotation_summary = mental_rotation_data %>%
  group_by(PROLIFIC_PID, Sentence, trial) %>%
  summarize(mean_angle = mean(angle, na.rm = T), mean_corr = mean(key_resp.corr, na.rm = T), num_rotations = n())
```

```{r}
mental_rotation_summary = mental_rotation_summary %>%
  rename(participant = PROLIFIC_PID)

data_analysis1_mental_rotation = data_analysis1_freq %>%
  left_join(mental_rotation_summary, by = c('participant', 'Sentence'))


```

```{r}
data_analysis1_mental_rotation = data_analysis1_mental_rotation %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc, mean_angle, mean_corr, num_rotations) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup() %>%
  group_by(participant) %>%
  mutate(ExpAcc = mean(correct))

sum(data_analysis1_mental_rotation['ExpAcc'] > 0.7)

mean_rts = data_analysis1_mental_rotation %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = data_analysis1_mental_rotation %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

data_analysis1_mental_rotation = data_analysis1_mental_rotation %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

data_analysis1_mental_rotation$prime_freq = relevel(data_analysis1_mental_rotation$prime_freq, ref = 'unrelated')
data_analysis1_mental_rotation$target_freq = relevel(data_analysis1_mental_rotation$target_freq, ref = 'frequent')

data_analysis1_mental_rotation$participant = factor(data_analysis1_mental_rotation$participant)
data_analysis1_mental_rotation$Item = factor(data_analysis1_mental_rotation$Item)

length(unique(data_analysis1_mental_rotation$participant))
length(unique(data_analysis1_mental_rotation$Item))

rows_per_participant = data_analysis1_mental_rotation %>% #sanity check
  group_by(participant) %>%
  summarize(n())
```

```{r}
for_plots = na.omit(data_analysis1_mental_rotation)
plot(for_plots$mean_angle, for_plots$mean_corr)
```

```{r}
easy_trials = for_plots %>%
  filter(mean_angle < 90)

easy_trials_model = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = easy_trials,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'easy_trials_model_freq' #accuracy_greater_90
)

fixef(easy_trials_model)
conditional_effects(easy_trials_model)
```

```{r}
percent_greater_zero = data.frame(fixef(easy_trials_model, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero



```

# Pilot Experiment 2

## Loading data

```{r}
data = read.csv('../Data/data_pilotexp2.csv', quote = "\"")


#pilot_data$date = as.Date(pilot_data$date)

data$date = as.Date(data$date)
# data = pilot_data %>%
#   filter(date > as.Date("2025-01-24"))

# pilot = pilot_data2 %>%
#   select('participant', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
#   filter(!is.na(List)) %>%
#   mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
#   group_by(participant) %>%
#   mutate('OverallAcc' = mean(correct))

data_analysis = data %>%
  dplyr::select('PROLIFIC_PID', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
  #filter(!is.na(List)) %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  rename('participant' = 'PROLIFIC_PID') %>%
  group_by(participant) %>%
  mutate('OverallAcc' = mean(correct)) 
  


audio_questions_key = read_csv('../Data/audio_questions.csv')
#audio_questions_key$Item = factor(audio_questions_key$Item)


data_analysis = data_analysis %>%
  left_join(audio_questions_key)

audio_only = data_analysis %>%
  filter(Question_about_audio == '1') %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  group_by(participant) %>%
  mutate('AudioAcc' = mean(correct)) %>%
  slice_head(n=1) %>%
  select(participant, AudioAcc)

data_analysis = data_analysis %>%
  left_join(audio_only)

# 

# data_analysis2 = pilot %>%
#   full_join(data_analysis)

# data_analysis = data_analysis2

data_analysis = data_analysis %>%
  mutate(sentence_split = str_split(Sentence, " "))

data_analysis = data_analysis %>%
  rowwise() %>%  # Ensures per-row operation
  mutate(target_word_index = case_when(
    SentenceType == 'Sentence (alphabetical)' ~ which(sentence_split == WordA)[1],
    SentenceType == 'Sentence (nonalphabetical)' ~ which(sentence_split == WordB)[1],
    TRUE ~ NA_real_
  )) %>%
  ungroup()  # Remove rowwise grouping




data_analysis = data_analysis %>%
  mutate(target_word_index = target_word_index - 1)



data_analysis1 = data_analysis %>%
  pivot_longer(cols = matches("^Word[0-9]+$"), names_to = 'word_index', names_prefix = 'Word', values_to = 'reading_time')



data_analysis1 = data_analysis1 %>%
  mutate(
    words = str_split(Sentence, "\\s+"),  # Split sentences into words
    extracted_word = mapply(function(word_list, idx) {
      adj_idx = as.numeric(idx) + 1  # Adjust for 0-based index
      if (adj_idx > length(word_list) | adj_idx < 1) NA else word_list[adj_idx]
    }, words, word_index)
  ) %>%
  select(-words)  # Drop the intermediate list column

data_analysis1$word_index = as.numeric(data_analysis1$word_index)
data_analysis1$target_word_index = as.numeric(data_analysis1$target_word_index)
data_analysis1$reading_time = data_analysis1$reading_time * 1000 #conver to ms

data_analysis1 = data_analysis1 %>%
  ungroup() %>%
  mutate('target' = ifelse(word_index >= target_word_index & word_index <= (target_word_index + 2), 1, 0)) %>%
  ungroup() %>%
  mutate('spillover' = ifelse(word_index >= (target_word_index + 3) & word_index <= (target_word_index + 5), 1, 0)) 

data_analysis1 = data_analysis1 %>%
  mutate(word_len = nchar(extracted_word))

data_analysis1 = data_analysis1 %>%
  filter(!is.na(reading_time))

data_analysis1$participant = factor(data_analysis1$participant)
data_analysis1$Item = factor(data_analysis1$Item)
data_analysis1$rt = data_analysis1$reading_time

data_analysis1 = data_analysis1 %>%
  filter(extracted_word != '+')



data_analysis1 <- data_analysis1 %>%
  arrange(Item, word_index) %>%
  group_by(Item) %>%
  slice(1:(n() - 1)) %>%  # Selects all but the last row per group
  ungroup()

data_analysis1$participant = factor(data_analysis1$participant)

residual_rts = data.frame(residual_rts = res.rt(data_analysis1, subj.name = 'participant', wordlen.name = 'word_len'))

data_analysis1$residual_rts = residual_rts$residual_rts


key = read_csv('../Data/key.csv')

key$Item = factor(key$Item)

  
data_analysis1 = data_analysis1 %>%
  left_join(key) 
  
length(unique(data_analysis1$Item))

data_analysis1$prime_freq = factor(data_analysis1$prime_freq, levels = c('unrelated', 'frequent'))
data_analysis1$target_freq = factor(data_analysis1$target_freq, levels = c('not-frequent', 'frequent'))


data_analysis_second_exp = data_analysis1
```

## analyses

### Audio Accuracy > 0.7

```{r}

audio_acc_greater_70 = data_analysis1 %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup()

mean_rts = audio_acc_greater_70 %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = audio_acc_greater_70 %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

audio_acc_greater_70 = audio_acc_greater_70 %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

audio_acc_greater_70$prime_freq = relevel(audio_acc_greater_70$prime_freq, ref = 'unrelated')
audio_acc_greater_70$target_freq = relevel(audio_acc_greater_70$target_freq, ref = 'not-frequent')

audio_acc_greater_70$participant = factor(audio_acc_greater_70$participant)
audio_acc_greater_70$Item = factor(audio_acc_greater_70$Item)

length(unique(audio_acc_greater_70$participant))
length(unique(audio_acc_greater_70$Item))



```
### Models

```{r}

audio_acc_70 = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = audio_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_pilotexp2' #accuracy_greater_90
)


audio_acc_70_prime = brm(residual_rts ~ 0 + Intercept + prime_freq + 
            (prime_freq | Item) + (prime_freq | participant),
          data = audio_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_prime_pilotexp2' #accuracy_greater_90
)

audio_acc_70_target = brm(residual_rts ~ 0 + Intercept + target_freq + 
            (target_freq | Item) + (target_freq | participant),
          data = audio_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_target_pilotexp2' #accuracy_greater_90
)



only_freq_target = audio_acc_greater_70 %>%
  filter(target_freq == 'frequent')

audio_acc_70_target_only_freq = brm(residual_rts ~ 0 + Intercept + prime_freq + 
            (prime_freq | Item) + (prime_freq | participant),
          data = only_freq_target,
          iter = 10000, 
          warmup = 5000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 50)", class = "b"),
                set_prior("student_t(3, 0, 50)", class = "sd")),
         control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_target_freq_pilotexp2' #accuracy_greater_90
)



percent_greater_zero = data.frame(fixef(audio_acc_70, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero


fixef(audio_acc_70)
fixef(audio_acc_70_prime)
fixef(audio_acc_70_target)
fixef(audio_acc_70_target_only_freq)


conditional_effects(audio_acc_70)
conditional_effects(audio_acc_70_prime)
conditional_effects(audio_acc_70_target)
conditional_effects(audio_acc_70_target_only_freq)


```
#### Sum coded

```{r}
options (contrasts = c('contr.sum','contr.sum'))


audio_acc_70_sum_coded = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = audio_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),        
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_pilotexp2_sum_coded' #accuracy_greater_90
)


audio_acc_70_prime_sum_coded = brm(residual_rts ~ 0 + Intercept + prime_freq + 
            (prime_freq | Item) + (prime_freq | participant),
          data = audio_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_prime_pilotexp2_sum_coded' #accuracy_greater_90
)

audio_acc_70_target_sum_coded = brm(residual_rts ~ 0 + Intercept + target_freq + 
            (target_freq | Item) + (target_freq | participant),
          data = audio_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_target_pilotexp2_sum_coded' #accuracy_greater_90
)



audio_acc_70_target_only_freq_sum_coded = brm(residual_rts ~ 0 + Intercept + prime_freq + 
            (prime_freq | Item) + (prime_freq | participant),
          data = only_freq_target,
          iter = 10000, 
          warmup = 5000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 50)", class = "b"),
                set_prior("student_t(3, 0, 50)", class = "sd")),
         control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_target_freq_pilotexp2_sum_coded' #accuracy_greater_90
)



percent_greater_zero = data.frame(fixef(audio_acc_70_sum_coded, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero


fixef(audio_acc_70_sum_coded)
fixef(audio_acc_70_prime_sum_coded)
fixef(audio_acc_70_target_sum_coded)
fixef(audio_acc_70_target_only_freq_sum_coded)


conditional_effects(audio_acc_70_sum_coded)
conditional_effects(audio_acc_70_prime_sum_coded)
conditional_effects(audio_acc_70_target_sum_coded)
conditional_effects(audio_acc_70_target_only_freq_sum_coded)



options (contrasts = c('contr.treatment','contr.treatment'))

```



### Overall Acc > 0.7

```{r}

overall_acc_greater_70 = data_analysis1 %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup()

mean_rts = overall_acc_greater_70 %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = overall_acc_greater_70 %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

overall_acc_greater_70 = overall_acc_greater_70 %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

overall_acc_greater_70$prime_freq = relevel(overall_acc_greater_70$prime_freq, ref = 'unrelated')
overall_acc_greater_70$target_freq = relevel(overall_acc_greater_70$target_freq, ref = 'not-frequent')

overall_acc_greater_70$participant = factor(overall_acc_greater_70$participant)
overall_acc_greater_70$Item = factor(overall_acc_greater_70$Item)

length(unique(overall_acc_greater_70$participant))
length(unique(overall_acc_greater_70$Item))



```
#### Models

```{r}

overall_acc_70 = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = overall_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'overall_accuracy_greater_70_pilotexp2' #accuracy_greater_90
)


overall_acc_70_prime = brm(residual_rts ~ 0 + Intercept + prime_freq + 
            (prime_freq | Item) + (prime_freq | participant),
          data = overall_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'overall_accuracy_greater_70_prime_pilotexp2' #accuracy_greater_90
)

overall_acc_70_target = brm(residual_rts ~ 0 + Intercept + target_freq + 
            (target_freq | Item) + (target_freq | participant),
          data = overall_acc_greater_70,
          iter = 8000, 
          warmup = 4000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         #control = list(adapt_delta = 0.99),
         file = 'overall_accuracy_greater_70_target_pilotexp2' #accuracy_greater_90
)



overall_acc_only_freq_target = overall_acc_greater_70 %>%
  filter(target_freq == 'frequent')

overall_acc_70_target_only_freq = brm(residual_rts ~ 0 + Intercept + prime_freq + 
            (prime_freq | Item) + (prime_freq | participant),
          data = overall_acc_only_freq_target,
          iter = 10000, 
          warmup = 5000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 50)", class = "b"),
                set_prior("student_t(3, 0, 50)", class = "sd")),
         control = list(adapt_delta = 0.99),
         file = 'overall_accuracy_greater_70_target_freq_pilotexp2' #accuracy_greater_90
)



percent_greater_zero = data.frame(fixef(audio_acc_70, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero


fixef(overall_acc_70)
fixef(overall_acc_70_prime)
fixef(overall_acc_70_target)
fixef(overall_acc_70_target_only_freq)


conditional_effects(overall_acc_70)
conditional_effects(overall_acc_70)
conditional_effects(overall_acc_70_prime)
conditional_effects(overall_acc_70_target)


```
# Aggregate data, word-by-word RTs

```{r}

first_data = data_analysis_first_exp %>% 
  left_join(mental_rotation_summary, by = c('participant', 'Sentence')) %>%
  filter(mean_angle < 90) %>%
  filter(!is.na(List)) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  select(participant, Condition, extracted_word, prime_freq, target_freq, residual_rts, word_index, target_word_index)

second_data = data_analysis_second_exp %>% 
  filter(!is.na(List)) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  select(participant, Condition, extracted_word, prime_freq, target_freq, residual_rts, word_index, target_word_index)

nrow(first_data)
nrow(second_data)

aggregate_data = first_data %>%
  full_join(second_data)

nrow(aggregate_data)



mean_rts = aggregate_data %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = aggregate_data %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

aggregate_data = aggregate_data %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

aggregate_data$prime_freq = relevel(aggregate_data$prime_freq, ref = 'unrelated')
aggregate_data$target_freq = relevel(aggregate_data$target_freq, ref = 'not-frequent')

aggregate_data$participant = factor(aggregate_data$participant)

length(unique(aggregate_data$participant))
length(unique(aggregate_data$Item))



aggregate_data = aggregate_data %>%
  mutate(distance_from_target_region = word_index - target_word_index)

word_by_word_rts = aggregate_data %>%
  filter(prime_freq %in% c('frequent', 'unrelated')) %>%
  group_by(prime_freq, target_freq, distance_from_target_region) %>%
  summarize(
    mean_rts = mean(residual_rts),
    sd_rts = sd(residual_rts),  
    n = n(),
    se = sd_rts / sqrt(n) # Compute the 95% CI
  )


word_by_word_rts$ci95_rts = word_by_word_rts$sd_rts / sqrt(word_by_word_rts$n) * 1.96

word_by_word_rts = word_by_word_rts %>%
  filter(distance_from_target_region > -5 & distance_from_target_region < 8)
  
word_by_word_rts$distance_from_target_region = factor(word_by_word_rts$distance_from_target_region)

ggplot(word_by_word_rts, aes(x = distance_from_target_region, y = mean_rts, color = prime_freq)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = mean_rts - sd_rts, ymax = mean_rts + sd_rts), width = 0.2, position = position_dodge(0.2)) +
  ylim(c(-200, 200)) +
  facet_wrap(~target_freq) + 
  #ylim(c(min(mean_rts), max(mean_rts))) +
  geom_segment(aes(x = as.factor(0), xend = as.factor(0), y = -200, yend = 200), 
               linetype = "dotted", color = "black", size = 1, alpha = 0.5) +
  geom_segment(aes(x = as.factor(5), xend = as.factor(5), y = -200, yend = 200), 
               linetype = "dotted", color = "black", size = 1, alpha = 0.5) +
  theme_bw()

ggplot(word_by_word_rts, aes(x = distance_from_target_region, y = mean_rts, color = target_freq)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = mean_rts - sd_rts, ymax = mean_rts + sd_rts), width = 0.2, position = position_dodge(0.2)) +
  ylim(c(-200, 200)) +
  facet_wrap(~prime_freq) + 
  #ylim(c(min(mean_rts), max(mean_rts))) +
  geom_segment(aes(x = as.factor(0), xend = as.factor(0), y = -200, yend = 200), 
               linetype = "dotted", color = "black", size = 1, alpha = 0.5) +
  geom_segment(aes(x = as.factor(5), xend = as.factor(5), y = -200, yend = 200), 
               linetype = "dotted", color = "black", size = 1, alpha = 0.5) +
  theme_bw()

```

```{r}
second_data_for_plot = second_data %>%
  mutate(distance_from_target_region = word_index - target_word_index)
  

mean_rts = second_data_for_plot %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = second_data_for_plot %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

second_data_for_plot = second_data_for_plot %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

word_by_word_rts_second_data = second_data_for_plot %>%
  filter(prime_freq %in% c('frequent', 'unrelated')) %>%
  group_by(prime_freq, target_freq, distance_from_target_region) %>%
  summarize(
    mean_rts = mean(residual_rts),
    sd_rts = sd(residual_rts),  
    n = n(),
    ci95 = sd_rts / sqrt(n) * 1.96  # Compute the 95% CI
  )


word_by_word_rts_second_data$ci95_rts = word_by_word_rts_second_data$sd_rts / sqrt(word_by_word_rts_second_data$n) * 1.96

ggplot(word_by_word_rts_second_data, aes(x = distance_from_target_region, y = mean_rts, color = prime_freq)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = mean_rts - sd_rts, ymax = mean_rts + sd_rts), width = 0.2, position = position_dodge(0.2)) +
  facet_wrap(~target_freq)

```
# Exp 3

```{r}
#pilot_data = read.csv('../Data/pilot_results3.csv', quote = "\"")
data = read.csv('../Data/exp3_sona.csv', quote = "\"")

data_pavlovia = read.csv('../Data/exp3_pavlovia.csv')

data$date = as.Date(data$date)
data_pavlovia$date = as.Date(data_pavlovia$date)

data_analysis_sona = data %>%
  dplyr::select('participant', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
  #filter(!is.na(List)) %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  #rename('participant' = 'PROLIFIC_PID') %>%
  group_by(participant) %>%
  mutate('OverallAcc' = mean(correct)) 
  
data_analysis_pavlovia = data_pavlovia %>%
  dplyr::select('PROLIFIC_PID', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
  #filter(!is.na(List)) %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  rename('participant' = 'PROLIFIC_PID') %>%
  group_by(participant) %>%
  mutate('OverallAcc' = mean(correct)) 

data_analysis_sona$participant = as.character(data_analysis_sona$participant)

data_analysis = data_analysis_sona %>%
  full_join(data_analysis_pavlovia)

nrow(data_analysis_sona) + nrow(data_analysis_pavlovia) == nrow(data_analysis)

audio_questions_key = read_csv('../Data/audio_questions.csv')
#audio_questions_key$Item = factor(audio_questions_key$Item)


data_analysis = data_analysis %>%
  left_join(audio_questions_key)

audio_only = data_analysis %>%
  filter(Question_about_audio == '1') %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  group_by(participant) %>%
  mutate('AudioAcc' = mean(correct)) %>%
  slice_head(n=1) %>%
  select(participant, AudioAcc)

data_analysis = data_analysis %>%
  left_join(audio_only)

# 

# data_analysis2 = pilot %>%
#   full_join(data_analysis)

# data_analysis = data_analysis2

data_analysis = data_analysis %>%
  mutate(sentence_split = str_split(Sentence, " "))

data_analysis = data_analysis %>%
  rowwise() %>%  # Ensures per-row operation
  mutate(target_word_index = case_when(
    SentenceType == 'Sentence (alphabetical)' ~ which(sentence_split == WordA)[1],
    SentenceType == 'Sentence (nonalphabetical)' ~ which(sentence_split == WordB)[1],
    TRUE ~ NA_real_
  )) %>%
  ungroup()  # Remove rowwise grouping




data_analysis = data_analysis %>%
  mutate(target_word_index = target_word_index - 1)



data_analysis1 = data_analysis %>%
  pivot_longer(cols = matches("^Word[0-9]+$"), names_to = 'word_index', names_prefix = 'Word', values_to = 'reading_time')



data_analysis1 = data_analysis1 %>%
  mutate(
    words = str_split(Sentence, "\\s+"),  # Split sentences into words
    extracted_word = mapply(function(word_list, idx) {
      adj_idx = as.numeric(idx) + 1  # Adjust for 0-based index
      if (adj_idx > length(word_list) | adj_idx < 1) NA else word_list[adj_idx]
    }, words, word_index)
  ) %>%
  select(-words)  # Drop the intermediate list column

data_analysis1$word_index = as.numeric(data_analysis1$word_index)
data_analysis1$target_word_index = as.numeric(data_analysis1$target_word_index)
data_analysis1$reading_time = data_analysis1$reading_time * 1000 #conver to ms

data_analysis1 = data_analysis1 %>%
  ungroup() %>%
  mutate('target' = ifelse(word_index >= target_word_index & word_index <= (target_word_index + 2), 1, 0)) %>%
  ungroup() %>%
  mutate('spillover' = ifelse(word_index >= (target_word_index + 3) & word_index <= (target_word_index + 5), 1, 0)) 

data_analysis1 = data_analysis1 %>%
  mutate(word_len = nchar(extracted_word))

data_analysis1 = data_analysis1 %>%
  filter(!is.na(reading_time))

data_analysis1$participant = factor(data_analysis1$participant)
data_analysis1$Item = factor(data_analysis1$Item)
data_analysis1$rt = data_analysis1$reading_time

data_analysis1 = data_analysis1 %>%
  filter(extracted_word != '+')



data_analysis1 <- data_analysis1 %>%
  arrange(Item, word_index) %>%
  group_by(Item) %>%
  slice(1:(n() - 1)) %>%  # Selects all but the last row per group
  ungroup()

data_analysis1$participant = factor(data_analysis1$participant)

residual_rts = data.frame(residual_rts = res.rt(data_analysis1, subj.name = 'participant', wordlen.name = 'word_len'))

data_analysis1$residual_rts = residual_rts$residual_rts


# 
# 
# summary_data = data_analysis1 %>%
#   group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct) %>%
#   summarize(
#     target_rt = sum(residual_rts[target == 1], na.rm = TRUE),
#     spillover_rt = sum(residual_rts[spillover == 1], na.rm = TRUE),
#     target_len = sum(word_len[target == 1], na.rm = T),
#     spillover_len = sum(word_len[spillover==1], na.rm = T),
#     .groups = "drop" 
#   )
# 
# summary_data = summary_data %>%
#   ungroup() %>%
#   group_by(participant, Item) %>%
#   pivot_longer(cols = c('target_rt', 'spillover_rt', 'target_len', 'spillover_len'),
#                names_to = c("region", ".value"),
#                names_pattern = "(.+)_(.+)")


# code to get residual rts
# res.rt <- function(dat, subj.name='subj', wordlen.name='wordlen',
#                         rt.name='rt', verbose=T) {
#   if(! is.element(wordlen.name,names(dat)))
#     stop(paste("Error: your data frame doesn't contain a compoment for wordlength named",wordlen.name))
#     dat <- data.frame(subj = dat[[subj.name]], wordlen = dat[[wordlen.name]],
#                       rt = dat[[rt.name]])
#     length.slope <- c() # just to see what typical slope is
#     res.rt <- c() # full list of residual rts
# 
#     for(i in levels(dat$subj)) {
#       wordlen.subj <- dat$wordlen[dat$subj==i]
#       if(length(wordlen.subj) == 0)
#         next
#       rt.subj <- dat$rt[dat$subj==i]
#       model <- lm(rt.subj~wordlen.subj)
#       res.rt[dat$subj==i] <- residuals(model)
#       length.slope <- c(length.slope, model$coef["wordlen.subj"])
#     }
#     mean.slope <- mean(length.slope)
#     se.slope <- se(length.slope)
#     if (verbose) {
#       cat(sprintf('The mean slope was %.1f with an se of %.1f.\n',
#                   mean.slope, se.slope))
#     }
#     return(res.rt)
# }




```

```{r}

key = read_csv('../Data/key.csv')

key$Item = factor(key$Item)

data_analysis_freq_primes = data_analysis1 %>%
  left_join(key) %>%
  filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  filter(prime_freq != 'infrequent') 
  
data_analysis1 = data_analysis1 %>%
  left_join(key) %>%
  filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  filter(prime_freq != 'frequent') 
  
length(unique(data_analysis1$Item))

  
```

```{r}

data_analysis1$prime_freq = factor(data_analysis1$prime_freq, levels = c('unrelated', 'not-frequent'))
data_analysis1$target_freq = factor(data_analysis1$target_freq, levels = c('not-frequent', 'frequent'))

```

## Unrelated vs not-frequent prime

### Overall Acc \> 70%

```{r}

target_region_data = data_analysis1 %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.85) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup() %>%
  group_by(participant) %>%
  mutate(ExpAcc = mean(correct))

sum(target_region_data['ExpAcc'] > 0.7)

mean_rts = target_region_data %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = target_region_data %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

target_region_data = target_region_data %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

target_region_data$prime_freq = relevel(target_region_data$prime_freq, ref = 'unrelated')
target_region_data$target_freq = relevel(target_region_data$target_freq, ref = 'not-frequent')

target_region_data$participant = factor(target_region_data$participant)
target_region_data$Item = factor(target_region_data$Item)

length(unique(target_region_data$participant))
length(unique(target_region_data$Item))

rows_per_participant = target_region_data %>% #sanity check
  group_by(participant) %>%
  summarize(n())

#using 0 + Intercept syntax following: https://discourse.mc-stan.org/t/understanding-intercept-prior-in-brms/34027
m1 = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = target_region_data,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(
                #set_prior("student_t(3, 0, 30)", class = "Intercept"),
                set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.95),
         file = 'full_m1_exp3' #accuracy_greater_90
)

fixef(m1)
conditional_effects(m1)

percent_greater_zero = data.frame(fixef(m1, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero

#fixef_df = data.frame(conditional_effects(m1)[1], plot=F)
```

### Audio Acc \> 70%

```{r}

audio_acc_greater_70 = data_analysis1 %>% 
  filter(!is.na(List)) %>%
  #filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup()

audio_acc_greater_70 %>%
  #group_by(participant) %>%
  summarize(mean(AudioAcc), mean(OverallAcc))

mean_rts = audio_acc_greater_70 %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = audio_acc_greater_70 %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

audio_acc_greater_70 = audio_acc_greater_70 %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

audio_acc_greater_70$prime_freq = relevel(audio_acc_greater_70$prime_freq, ref = 'unrelated')
audio_acc_greater_70$target_freq = relevel(audio_acc_greater_70$target_freq, ref = 'not-frequent')

audio_acc_greater_70$participant = factor(audio_acc_greater_70$participant)
audio_acc_greater_70$Item = factor(audio_acc_greater_70$Item)

length(unique(audio_acc_greater_70$participant))
length(unique(audio_acc_greater_70$Item))


audio_acc_70 = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = audio_acc_greater_70,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_exp3' #accuracy_greater_90
)

fixef(audio_acc_70)
conditional_effects(audio_acc_70)

percent_greater_zero = data.frame(fixef(audio_acc_70, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero
```
### Mental rotation performance

```{r}
mental_rotation_data = data %>% 
  select(participant, Prime, Sentence, angle, right_im, key_resp.corr, audio_loop.thisTrialN, audio_loop.thisIndex, audio_loop.thisN) %>%
  mutate(Sentence = na_if(Sentence, "")) %>%
  group_by(participant) %>%
  fill(Sentence, .direction = 'up')

mental_rotation_data = mental_rotation_data %>%
  group_by(participant, Sentence) %>%
  mutate(trial = audio_loop.thisTrialN) %>%
  fill(trial, .direction = 'up') 

 mental_rotation_data_part_excluded = mental_rotation_data %>%
   filter(participant %in% audio_acc_greater_70$participant)%>% 
   group_by(angle) %>% summarize(mean(key_resp.corr, na.rm = T))

mental_rotation_summary = mental_rotation_data %>%
  group_by(participant, Sentence, trial) %>%
  summarize(mean_angle = mean(angle, na.rm = T), mean_corr = mean(key_resp.corr, na.rm = T), num_rotations = n())
```

```{r}
mental_rotation_summary %>%
  filter(participant %in% audio_acc_greater_70$participant) %>%
  group_by(participant) %>%
  summarize(trial_mean_corr = mean(mean_corr, na.rm = T)) %>%
  ungroup() %>%
  summarize(mean(trial_mean_corr, na.rm = T))

mental_rotation_summary$participant = factor(mental_rotation_summary$participant)


mental_rotation_summary


```

## Unrelated vs Frequet Prime

```{r}
#pilot_data = read.csv('../Data/pilot_results3.csv', quote = "\"")
data = read.csv('../Data/exp3_sona.csv', quote = "\"")

data_pavlovia = read.csv('../Data/exp3_pavlovia.csv')

data$date = as.Date(data$date)
data_pavlovia$date = as.Date(data_pavlovia$date)

data_analysis_sona = data %>%
  dplyr::select('participant', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
  #filter(!is.na(List)) %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  #rename('participant' = 'PROLIFIC_PID') %>%
  group_by(participant) %>%
  mutate('OverallAcc' = mean(correct)) 
  
data_analysis_pavlovia = data_pavlovia %>%
  dplyr::select('PROLIFIC_PID', 'Item', 'List', 'Condition', 'WordA', 'WordB', 'Binomial', 'PrimeType', 'Prime', 'Sentence', 'SentenceType', 'Questions', 'Answer', 'key_resp.corr', 'key_resp_4.keys', 'trials.thisN', starts_with("Word")) %>%
  #filter(!is.na(List)) %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  rename('participant' = 'PROLIFIC_PID') %>%
  group_by(participant) %>%
  mutate('OverallAcc' = mean(correct)) 

data_analysis_sona$participant = as.character(data_analysis_sona$participant)

data_analysis = data_analysis_sona %>%
  full_join(data_analysis_pavlovia)

nrow(data_analysis_sona) + nrow(data_analysis_pavlovia) == nrow(data_analysis)


audio_questions_key = read_csv('../Data/audio_questions.csv')
#audio_questions_key$Item = factor(audio_questions_key$Item)


data_analysis = data_analysis %>%
  left_join(audio_questions_key)

audio_only = data_analysis %>%
  filter(Question_about_audio == '1') %>%
  mutate(correct = ifelse(`key_resp_4.keys` == Answer, 1, 0)) %>%
  group_by(participant) %>%
  mutate('AudioAcc' = mean(correct)) %>%
  slice_head(n=1) %>%
  select(participant, AudioAcc)

data_analysis = data_analysis %>%
  left_join(audio_only)

# 

# data_analysis2 = pilot %>%
#   full_join(data_analysis)

# data_analysis = data_analysis2

data_analysis = data_analysis %>%
  mutate(sentence_split = str_split(Sentence, " "))

data_analysis = data_analysis %>%
  rowwise() %>%  # Ensures per-row operation
  mutate(target_word_index = case_when(
    SentenceType == 'Sentence (alphabetical)' ~ which(sentence_split == WordA)[1],
    SentenceType == 'Sentence (nonalphabetical)' ~ which(sentence_split == WordB)[1],
    TRUE ~ NA_real_
  )) %>%
  ungroup()  # Remove rowwise grouping




data_analysis = data_analysis %>%
  mutate(target_word_index = target_word_index - 1)



data_analysis1_freq = data_analysis %>%
  pivot_longer(cols = matches("^Word[0-9]+$"), names_to = 'word_index', names_prefix = 'Word', values_to = 'reading_time')



data_analysis1_freq = data_analysis1_freq %>%
  mutate(
    words = str_split(Sentence, "\\s+"),  # Split sentences into words
    extracted_word = mapply(function(word_list, idx) {
      adj_idx = as.numeric(idx) + 1  # Adjust for 0-based index
      if (adj_idx > length(word_list) | adj_idx < 1) NA else word_list[adj_idx]
    }, words, word_index)
  ) %>%
  select(-words)  # Drop the intermediate list column

data_analysis1_freq$word_index = as.numeric(data_analysis1_freq$word_index)
data_analysis1_freq$target_word_index = as.numeric(data_analysis1_freq$target_word_index)
data_analysis1_freq$reading_time = data_analysis1_freq$reading_time * 1000 #conver to ms

data_analysis1_freq = data_analysis1_freq %>%
  ungroup() %>%
  mutate('target' = ifelse(word_index >= target_word_index & word_index <= (target_word_index + 2), 1, 0)) %>%
  ungroup() %>%
  mutate('spillover' = ifelse(word_index >= (target_word_index + 3) & word_index <= (target_word_index + 5), 1, 0)) 

data_analysis1_freq = data_analysis1_freq %>%
  mutate(word_len = nchar(extracted_word))

data_analysis1_freq = data_analysis1_freq %>%
  filter(!is.na(reading_time))

data_analysis1_freq$participant = factor(data_analysis1_freq$participant)
data_analysis1_freq$Item = factor(data_analysis1_freq$Item)
data_analysis1_freq$rt = data_analysis1_freq$reading_time

data_analysis1_freq = data_analysis1_freq %>%
  filter(extracted_word != '+')



data_analysis1_freq <- data_analysis1_freq %>%
  arrange(Item, word_index) %>%
  group_by(Item) %>%
  slice(1:(n() - 1)) %>%  # Selects all but the last row per group
  ungroup()

data_analysis1_freq$participant = factor(data_analysis1_freq$participant)

residual_rts = data.frame(residual_rts = res.rt(data_analysis1_freq, subj.name = 'participant', wordlen.name = 'word_len'))

data_analysis1_freq$residual_rts = residual_rts$residual_rts

key = read_csv('../Data/key.csv')

key$Item = factor(key$Item)

  
data_analysis1_freq = data_analysis1_freq %>%
  left_join(key) %>%
  filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  filter(prime_freq != 'not-frequent') 
  
length(unique(data_analysis1_freq$Item))



data_analysis1_freq$prime_freq = factor(data_analysis1_freq$prime_freq, levels = c('unrelated', 'frequent'))
data_analysis1_freq$target_freq = factor(data_analysis1_freq$target_freq, levels = c('not-frequent', 'frequent'))




```

### Overall Acc \> 0.7

```{r}

target_region_data_freq = data_analysis1_freq %>% 
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  filter(OverallAcc > 0.7) %>% #exclude low-accuracy participants
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup() %>%
  group_by(participant) %>%
  mutate(ExpAcc = mean(correct))

sum(target_region_data_freq['ExpAcc'] > 0.7)

mean_rts = target_region_data_freq %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = target_region_data_freq %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

target_region_data_freq = target_region_data_freq %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

target_region_data_freq$prime_freq = relevel(target_region_data_freq$prime_freq, ref = 'unrelated')
target_region_data_freq$target_freq = relevel(target_region_data_freq$target_freq, ref = 'not-frequent')

target_region_data_freq$participant = factor(target_region_data_freq$participant)
target_region_data_freq$Item = factor(target_region_data_freq$Item)

length(unique(target_region_data_freq$participant))
length(unique(target_region_data_freq$Item))

rows_per_participant = target_region_data_freq %>% #sanity check
  group_by(participant) %>%
  summarize(n())
#using 0 + Intercept syntax following: https://discourse.mc-stan.org/t/understanding-intercept-prior-in-brms/34027
m1_freq = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = target_region_data_freq,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(
                #set_prior("student_t(3, 0, 30)", class = "Intercept"),
                set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.95),
         file = 'full_m1_freq_exp3' #accuracy_greater_90
)

fixef(m1_freq)
conditional_effects(m1_freq)

percent_greater_zero = data.frame(fixef(m1_freq, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero
```

### Audio Acc \> 0.7

```{r}

audio_acc_greater_70_freq = data_analysis1_freq %>% 
  #filter(AudioAcc > 0.7) %>% #exclude low-accuracy participants
  filter(!is.na(List)) %>%
  filter(target == 1 | spillover == 1) %>%
  filter(rt > 100 & rt < 5000) %>%
  group_by(participant, Item, List, Condition, WordA, WordB, Binomial, PrimeType, Prime, Sentence, SentenceType, Questions, Answer, OverallAcc, correct, prime_freq, target_freq, AudioAcc) %>%
  summarize(residual_rts = sum(residual_rts)) %>%
  ungroup()

audio_acc_greater_70_freq %>%
  #group_by(participant) %>%
  summarize(mean(AudioAcc), mean(OverallAcc))

mean_rts = audio_acc_greater_70_freq %>%
  ungroup() %>%
  summarize(mean(residual_rts))

sd_rts = audio_acc_greater_70_freq %>%
  ungroup() %>%
  summarize(sd(residual_rts))

lower_bound = as.double(mean_rts[1,1]) - 2.5 * as.double(sd_rts[1,1])
upper_bound = as.double(mean_rts[1,1]) + 2.5 * as.double(sd_rts[1,1])

audio_acc_greater_70_freq = audio_acc_greater_70_freq %>%
  filter((residual_rts > lower_bound) & (residual_rts < upper_bound))

  

audio_acc_greater_70_freq$prime_freq = relevel(audio_acc_greater_70_freq$prime_freq, ref = 'unrelated')
audio_acc_greater_70_freq$target_freq = relevel(audio_acc_greater_70_freq$target_freq, ref = 'not-frequent')

audio_acc_greater_70_freq$participant = factor(audio_acc_greater_70_freq$participant)
audio_acc_greater_70_freq$Item = factor(audio_acc_greater_70_freq$Item)

length(unique(audio_acc_greater_70_freq$participant))
length(unique(audio_acc_greater_70_freq$Item))


audio_acc_70_freq = brm(residual_rts ~ 0 + Intercept + prime_freq * target_freq + 
            (prime_freq * target_freq | Item) + (prime_freq * target_freq | participant),
          data = audio_acc_greater_70_freq,
          iter = 30000, 
          warmup = 15000,
          chains = 4,
          core = 4,
          thin = 4,
          prior = c(set_prior("student_t(3, 0, 30)", class = "b"),
                set_prior("student_t(3, 0, 30)", class = "sd")),
         control = list(adapt_delta = 0.99),
         file = 'audio_accuracy_greater_70_freq_exp3' #accuracy_greater_90
)

fixef(audio_acc_70_freq)
conditional_effects(audio_acc_70_freq)

percent_greater_zero = data.frame(fixef(audio_acc_70_freq, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero
```

### Mental rotation performance

```{r}
mental_rotation_data = data %>% 
  select(participant, Prime, Sentence, angle, right_im, key_resp.corr, audio_loop.thisTrialN, audio_loop.thisIndex, audio_loop.thisN) %>%
  mutate(Sentence = na_if(Sentence, "")) %>%
  group_by(participant) %>%
  fill(Sentence, .direction = 'up')

mental_rotation_data = mental_rotation_data %>%
  group_by(participant, Sentence) %>%
  mutate(trial = audio_loop.thisTrialN) %>%
  fill(trial, .direction = 'up') 

 mental_rotation_data_part_excluded_freq = mental_rotation_data %>%
   filter(participant %in% audio_acc_greater_70_freq$participant)%>% 
   group_by(angle) %>% summarize(mean(key_resp.corr, na.rm = T))

mental_rotation_summary = mental_rotation_data %>%
  group_by(participant, Sentence, trial) %>%
  summarize(mean_angle = mean(angle, na.rm = T), mean_corr = mean(key_resp.corr, na.rm = T), num_rotations = n())
```

```{r}
mental_rotation_summary %>%
  filter(participant %in% audio_acc_greater_70_freq$participant) %>%
  group_by(participant) %>%
  summarize(trial_mean_corr = mean(mean_corr, na.rm = T)) %>%
  ungroup() %>%
  summarize(mean(trial_mean_corr, na.rm = T))

mental_rotation_summary$participant = factor(mental_rotation_summary$participant)


mental_rotation_summary


```

## Plots


```{r}
library(ggpubr)

p1 = plot(conditional_effects(audio_acc_70_freq), plot = F)[[3]] +
  theme_bw()

p2 = plot(conditional_effects(audio_acc_70), plot = F)[[3]] +
  theme_bw()

ggarrange(p1, p2, common.legend = T)

```

